{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvcc —-version"
      ],
      "metadata": {
        "id": "rJ7p4W5HwHiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n"
      ],
      "metadata": {
        "id": "3O6rYuK6SiIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mnzDv5EHICQv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "992983a1-ddcd-400a-89e9-e34bc93cea3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Training with learning rate = 0.001000, momentum = 0.900000 \n",
            "Epoch, batch [1,  2000] loss: 2.302750, Training accuracy: 9.92500\n",
            "Epoch, batch [1,  4000] loss: 2.303375, Training accuracy: 9.80000\n",
            "Epoch, batch [1,  6000] loss: 2.302923, Training accuracy: 10.15000\n",
            "Epoch, batch [1,  8000] loss: 2.303301, Training accuracy: 9.97500\n",
            "Epoch, batch [1, 10000] loss: 2.303021, Training accuracy: 9.95000\n",
            "Epoch, batch [1, 12000] loss: 2.297798, Training accuracy: 10.53750\n",
            "Epoch, batch [2,  2000] loss: 2.145198, Training accuracy: 18.15000\n",
            "Epoch, batch [2,  4000] loss: 2.035734, Training accuracy: 17.93750\n",
            "Epoch, batch [2,  6000] loss: 1.958960, Training accuracy: 19.02500\n",
            "Epoch, batch [2,  8000] loss: 1.925082, Training accuracy: 20.06250\n",
            "Epoch, batch [2, 10000] loss: 1.893983, Training accuracy: 21.73750\n",
            "Epoch, batch [2, 12000] loss: 1.830274, Training accuracy: 25.96250\n",
            "Epoch, batch [3,  2000] loss: 1.744843, Training accuracy: 29.73750\n",
            "Epoch, batch [3,  4000] loss: 1.697200, Training accuracy: 31.38750\n",
            "Epoch, batch [3,  6000] loss: 1.676681, Training accuracy: 33.35000\n",
            "Epoch, batch [3,  8000] loss: 1.644495, Training accuracy: 34.57500\n",
            "Epoch, batch [3, 10000] loss: 1.587335, Training accuracy: 37.01250\n",
            "Epoch, batch [3, 12000] loss: 1.564746, Training accuracy: 39.40000\n",
            "Epoch, batch [4,  2000] loss: 1.470193, Training accuracy: 43.87500\n",
            "Epoch, batch [4,  4000] loss: 1.434650, Training accuracy: 45.20000\n",
            "Epoch, batch [4,  6000] loss: 1.408118, Training accuracy: 46.45000\n",
            "Epoch, batch [4,  8000] loss: 1.334626, Training accuracy: 50.58750\n",
            "Epoch, batch [4, 10000] loss: 1.297790, Training accuracy: 52.61250\n",
            "Epoch, batch [4, 12000] loss: 1.236398, Training accuracy: 55.80000\n",
            "Epoch, batch [5,  2000] loss: 1.137995, Training accuracy: 58.73750\n",
            "Epoch, batch [5,  4000] loss: 1.145839, Training accuracy: 58.80000\n",
            "Epoch, batch [5,  6000] loss: 1.105985, Training accuracy: 61.25000\n",
            "Epoch, batch [5,  8000] loss: 1.060296, Training accuracy: 61.82500\n",
            "Epoch, batch [5, 10000] loss: 1.056705, Training accuracy: 63.01250\n",
            "Epoch, batch [5, 12000] loss: 1.010625, Training accuracy: 65.38750\n",
            "Epoch, batch [6,  2000] loss: 0.959784, Training accuracy: 66.43750\n",
            "Epoch, batch [6,  4000] loss: 0.924715, Training accuracy: 68.25000\n",
            "Epoch, batch [6,  6000] loss: 0.914387, Training accuracy: 68.37500\n",
            "Epoch, batch [6,  8000] loss: 0.896898, Training accuracy: 69.86250\n",
            "Epoch, batch [6, 10000] loss: 0.895651, Training accuracy: 69.15000\n",
            "Epoch, batch [6, 12000] loss: 0.877761, Training accuracy: 69.85000\n",
            "Epoch, batch [7,  2000] loss: 0.774687, Training accuracy: 74.02500\n",
            "Epoch, batch [7,  4000] loss: 0.800151, Training accuracy: 72.68750\n",
            "Epoch, batch [7,  6000] loss: 0.812339, Training accuracy: 72.98750\n",
            "Epoch, batch [7,  8000] loss: 0.791783, Training accuracy: 73.63750\n",
            "Epoch, batch [7, 10000] loss: 0.790027, Training accuracy: 73.37500\n",
            "Epoch, batch [7, 12000] loss: 0.781826, Training accuracy: 73.80000\n",
            "Epoch, batch [8,  2000] loss: 0.679946, Training accuracy: 77.25000\n",
            "Epoch, batch [8,  4000] loss: 0.704426, Training accuracy: 76.78750\n",
            "Epoch, batch [8,  6000] loss: 0.698214, Training accuracy: 77.30000\n",
            "Epoch, batch [8,  8000] loss: 0.704980, Training accuracy: 76.65000\n",
            "Epoch, batch [8, 10000] loss: 0.678188, Training accuracy: 77.35000\n",
            "Epoch, batch [8, 12000] loss: 0.710620, Training accuracy: 76.55000\n",
            "Epoch, batch [9,  2000] loss: 0.605222, Training accuracy: 80.26250\n",
            "Epoch, batch [9,  4000] loss: 0.605868, Training accuracy: 80.15000\n",
            "Epoch, batch [9,  6000] loss: 0.642415, Training accuracy: 78.22500\n",
            "Epoch, batch [9,  8000] loss: 0.644056, Training accuracy: 78.90000\n",
            "Epoch, batch [9, 10000] loss: 0.619006, Training accuracy: 79.85000\n",
            "Epoch, batch [9, 12000] loss: 0.623650, Training accuracy: 79.46250\n",
            "Epoch, batch [10,  2000] loss: 0.520286, Training accuracy: 82.52500\n",
            "Epoch, batch [10,  4000] loss: 0.546582, Training accuracy: 82.32500\n",
            "Epoch, batch [10,  6000] loss: 0.546971, Training accuracy: 81.96250\n",
            "Epoch, batch [10,  8000] loss: 0.553539, Training accuracy: 81.83750\n",
            "Epoch, batch [10, 10000] loss: 0.551705, Training accuracy: 81.95000\n",
            "Epoch, batch [10, 12000] loss: 0.576643, Training accuracy: 81.31250\n",
            "Epoch, batch [11,  2000] loss: 0.447931, Training accuracy: 85.21250\n",
            "Epoch, batch [11,  4000] loss: 0.495554, Training accuracy: 83.46250\n",
            "Epoch, batch [11,  6000] loss: 0.491369, Training accuracy: 83.71250\n",
            "Epoch, batch [11,  8000] loss: 0.506231, Training accuracy: 83.23750\n",
            "Epoch, batch [11, 10000] loss: 0.503928, Training accuracy: 83.33750\n",
            "Epoch, batch [11, 12000] loss: 0.523449, Training accuracy: 82.57500\n",
            "Epoch, batch [12,  2000] loss: 0.411027, Training accuracy: 86.87500\n",
            "Epoch, batch [12,  4000] loss: 0.433097, Training accuracy: 85.67500\n",
            "Epoch, batch [12,  6000] loss: 0.452450, Training accuracy: 85.72500\n",
            "Epoch, batch [12,  8000] loss: 0.462261, Training accuracy: 85.00000\n",
            "Epoch, batch [12, 10000] loss: 0.472746, Training accuracy: 84.53750\n",
            "Epoch, batch [12, 12000] loss: 0.443589, Training accuracy: 85.70000\n",
            "Epoch, batch [13,  2000] loss: 0.389992, Training accuracy: 87.02500\n",
            "Epoch, batch [13,  4000] loss: 0.401897, Training accuracy: 86.92500\n",
            "Epoch, batch [13,  6000] loss: 0.434109, Training accuracy: 85.88750\n",
            "Epoch, batch [13,  8000] loss: 0.421535, Training accuracy: 86.23750\n",
            "Epoch, batch [13, 10000] loss: 0.429599, Training accuracy: 85.87500\n",
            "Epoch, batch [13, 12000] loss: 0.412698, Training accuracy: 86.38750\n",
            "Epoch, batch [14,  2000] loss: 0.340564, Training accuracy: 88.83750\n",
            "Epoch, batch [14,  4000] loss: 0.360280, Training accuracy: 88.22500\n",
            "Epoch, batch [14,  6000] loss: 0.392742, Training accuracy: 87.41250\n",
            "Epoch, batch [14,  8000] loss: 0.358672, Training accuracy: 88.53750\n",
            "Epoch, batch [14, 10000] loss: 0.405578, Training accuracy: 86.68750\n",
            "Epoch, batch [14, 12000] loss: 0.408423, Training accuracy: 86.82500\n",
            "Epoch, batch [15,  2000] loss: 0.317327, Training accuracy: 89.81250\n",
            "Epoch, batch [15,  4000] loss: 0.321116, Training accuracy: 89.38750\n",
            "Epoch, batch [15,  6000] loss: 0.340862, Training accuracy: 89.00000\n",
            "Epoch, batch [15,  8000] loss: 0.358619, Training accuracy: 88.42500\n",
            "Epoch, batch [15, 10000] loss: 0.358067, Training accuracy: 88.38750\n",
            "Epoch, batch [15, 12000] loss: 0.372338, Training accuracy: 88.12500\n",
            "Epoch, batch [16,  2000] loss: 0.277650, Training accuracy: 90.96250\n",
            "Epoch, batch [16,  4000] loss: 0.294682, Training accuracy: 90.48750\n",
            "Epoch, batch [16,  6000] loss: 0.307366, Training accuracy: 89.90000\n",
            "Epoch, batch [16,  8000] loss: 0.308212, Training accuracy: 90.26250\n",
            "Epoch, batch [16, 10000] loss: 0.319055, Training accuracy: 89.88750\n",
            "Epoch, batch [16, 12000] loss: 0.310590, Training accuracy: 89.41250\n",
            "Epoch, batch [17,  2000] loss: 0.265362, Training accuracy: 91.46250\n",
            "Epoch, batch [17,  4000] loss: 0.295964, Training accuracy: 90.07500\n",
            "Epoch, batch [17,  6000] loss: 0.295234, Training accuracy: 90.85000\n",
            "Epoch, batch [17,  8000] loss: 0.314482, Training accuracy: 90.08750\n",
            "Epoch, batch [17, 10000] loss: 0.318459, Training accuracy: 89.91250\n",
            "Epoch, batch [17, 12000] loss: 0.302060, Training accuracy: 90.35000\n",
            "Epoch, batch [18,  2000] loss: 0.245108, Training accuracy: 92.43750\n",
            "Epoch, batch [18,  4000] loss: 0.262521, Training accuracy: 91.57500\n",
            "Epoch, batch [18,  6000] loss: 0.264672, Training accuracy: 91.55000\n",
            "Epoch, batch [18,  8000] loss: 0.276720, Training accuracy: 90.97500\n",
            "Epoch, batch [18, 10000] loss: 0.308757, Training accuracy: 90.06250\n",
            "Epoch, batch [18, 12000] loss: 0.307718, Training accuracy: 90.07500\n",
            "Epoch, batch [19,  2000] loss: 0.236114, Training accuracy: 92.40000\n",
            "Epoch, batch [19,  4000] loss: 0.241389, Training accuracy: 92.02500\n",
            "Epoch, batch [19,  6000] loss: 0.292361, Training accuracy: 90.76250\n",
            "Epoch, batch [19,  8000] loss: 0.285458, Training accuracy: 90.92500\n",
            "Epoch, batch [19, 10000] loss: 0.278440, Training accuracy: 91.27500\n",
            "Epoch, batch [19, 12000] loss: 0.270506, Training accuracy: 91.43750\n",
            "Epoch, batch [20,  2000] loss: 0.207327, Training accuracy: 93.52500\n",
            "Epoch, batch [20,  4000] loss: 0.261652, Training accuracy: 92.15000\n",
            "Epoch, batch [20,  6000] loss: 0.245676, Training accuracy: 92.37500\n",
            "Epoch, batch [20,  8000] loss: 0.278574, Training accuracy: 91.08750\n",
            "Epoch, batch [20, 10000] loss: 0.270700, Training accuracy: 91.17500\n",
            "Epoch, batch [20, 12000] loss: 0.287667, Training accuracy: 91.11250\n",
            "Epoch, batch [21,  2000] loss: 0.215816, Training accuracy: 93.55000\n",
            "Epoch, batch [21,  4000] loss: 0.235452, Training accuracy: 92.88750\n",
            "Epoch, batch [21,  6000] loss: 0.239881, Training accuracy: 92.58750\n",
            "Epoch, batch [21,  8000] loss: 0.222512, Training accuracy: 92.93750\n",
            "Epoch, batch [21, 10000] loss: 0.245764, Training accuracy: 92.13750\n",
            "Epoch, batch [21, 12000] loss: 0.274656, Training accuracy: 91.28750\n",
            "Epoch, batch [22,  2000] loss: 0.186577, Training accuracy: 94.33750\n",
            "Epoch, batch [22,  4000] loss: 0.216828, Training accuracy: 93.17500\n",
            "Epoch, batch [22,  6000] loss: 0.195044, Training accuracy: 93.86250\n",
            "Epoch, batch [22,  8000] loss: 0.230800, Training accuracy: 92.78750\n",
            "Epoch, batch [22, 10000] loss: 0.226572, Training accuracy: 92.91250\n",
            "Epoch, batch [22, 12000] loss: 0.267728, Training accuracy: 91.75000\n",
            "Epoch, batch [23,  2000] loss: 0.194103, Training accuracy: 94.13750\n",
            "Epoch, batch [23,  4000] loss: 0.193564, Training accuracy: 94.18750\n",
            "Epoch, batch [23,  6000] loss: 0.229061, Training accuracy: 92.92500\n",
            "Epoch, batch [23,  8000] loss: 0.228544, Training accuracy: 93.06250\n",
            "Epoch, batch [23, 10000] loss: 0.225334, Training accuracy: 93.08750\n",
            "Epoch, batch [23, 12000] loss: 0.265882, Training accuracy: 91.75000\n",
            "Epoch, batch [24,  2000] loss: 0.188054, Training accuracy: 94.20000\n",
            "Epoch, batch [24,  4000] loss: 0.224002, Training accuracy: 93.02500\n",
            "Epoch, batch [24,  6000] loss: 0.224975, Training accuracy: 93.20000\n",
            "Epoch, batch [24,  8000] loss: 0.235995, Training accuracy: 92.76250\n",
            "Epoch, batch [24, 10000] loss: 0.234627, Training accuracy: 92.85000\n",
            "Epoch, batch [24, 12000] loss: 0.219758, Training accuracy: 92.71250\n",
            "Epoch, batch [25,  2000] loss: 0.176888, Training accuracy: 94.58750\n",
            "Epoch, batch [25,  4000] loss: 0.199497, Training accuracy: 93.73750\n",
            "Epoch, batch [25,  6000] loss: 0.211279, Training accuracy: 93.66250\n",
            "Epoch, batch [25,  8000] loss: 0.233321, Training accuracy: 92.56250\n",
            "Epoch, batch [25, 10000] loss: 0.222235, Training accuracy: 93.06250\n",
            "Epoch, batch [25, 12000] loss: 0.251660, Training accuracy: 92.30000\n",
            "Epoch, batch [26,  2000] loss: 0.120976, Training accuracy: 96.37500\n",
            "Epoch, batch [26,  4000] loss: 0.092140, Training accuracy: 97.28750\n",
            "Epoch, batch [26,  6000] loss: 0.073121, Training accuracy: 97.68750\n",
            "Epoch, batch [26,  8000] loss: 0.064769, Training accuracy: 98.02500\n",
            "Epoch, batch [26, 10000] loss: 0.065807, Training accuracy: 98.07500\n",
            "Epoch, batch [26, 12000] loss: 0.065540, Training accuracy: 98.26250\n",
            "Epoch, batch [27,  2000] loss: 0.030120, Training accuracy: 99.28750\n",
            "Epoch, batch [27,  4000] loss: 0.025803, Training accuracy: 99.25000\n",
            "Epoch, batch [27,  6000] loss: 0.023278, Training accuracy: 99.42500\n",
            "Epoch, batch [27,  8000] loss: 0.023688, Training accuracy: 99.36250\n",
            "Epoch, batch [27, 10000] loss: 0.019990, Training accuracy: 99.40000\n",
            "Epoch, batch [27, 12000] loss: 0.022824, Training accuracy: 99.37500\n",
            "Epoch, batch [28,  2000] loss: 0.012108, Training accuracy: 99.75000\n",
            "Epoch, batch [28,  4000] loss: 0.011825, Training accuracy: 99.71250\n",
            "Epoch, batch [28,  6000] loss: 0.008918, Training accuracy: 99.83750\n",
            "Epoch, batch [28,  8000] loss: 0.008711, Training accuracy: 99.81250\n",
            "Epoch, batch [28, 10000] loss: 0.008999, Training accuracy: 99.80000\n",
            "Epoch, batch [28, 12000] loss: 0.008377, Training accuracy: 99.82500\n",
            "Epoch, batch [29,  2000] loss: 0.004687, Training accuracy: 99.91250\n",
            "Epoch, batch [29,  4000] loss: 0.005851, Training accuracy: 99.88750\n",
            "Epoch, batch [29,  6000] loss: 0.004512, Training accuracy: 99.93750\n",
            "Epoch, batch [29,  8000] loss: 0.006202, Training accuracy: 99.82500\n",
            "Epoch, batch [29, 10000] loss: 0.005877, Training accuracy: 99.90000\n",
            "Epoch, batch [29, 12000] loss: 0.006366, Training accuracy: 99.88750\n",
            "Epoch, batch [30,  2000] loss: 0.005682, Training accuracy: 99.88750\n",
            "Epoch, batch [30,  4000] loss: 0.002984, Training accuracy: 99.91250\n",
            "Epoch, batch [30,  6000] loss: 0.004976, Training accuracy: 99.86250\n",
            "Epoch, batch [30,  8000] loss: 0.004703, Training accuracy: 99.91250\n",
            "Epoch, batch [30, 10000] loss: 0.004016, Training accuracy: 99.91250\n",
            "Epoch, batch [30, 12000] loss: 0.003491, Training accuracy: 99.95000\n",
            "Epoch, batch [31,  2000] loss: 0.002486, Training accuracy: 99.93750\n",
            "Epoch, batch [31,  4000] loss: 0.004328, Training accuracy: 99.92500\n",
            "Epoch, batch [31,  6000] loss: 0.002396, Training accuracy: 99.97500\n",
            "Epoch, batch [31,  8000] loss: 0.003119, Training accuracy: 99.93750\n",
            "Epoch, batch [31, 10000] loss: 0.001897, Training accuracy: 99.96250\n",
            "Epoch, batch [31, 12000] loss: 0.003719, Training accuracy: 99.87500\n",
            "Epoch, batch [32,  2000] loss: 0.001133, Training accuracy: 99.98750\n",
            "Epoch, batch [32,  4000] loss: 0.000987, Training accuracy: 99.98750\n",
            "Epoch, batch [32,  6000] loss: 0.003633, Training accuracy: 99.88750\n",
            "Epoch, batch [32,  8000] loss: 0.002645, Training accuracy: 99.93750\n",
            "Epoch, batch [32, 10000] loss: 0.003827, Training accuracy: 99.91250\n",
            "Epoch, batch [32, 12000] loss: 0.001603, Training accuracy: 99.97500\n",
            "Epoch, batch [33,  2000] loss: 0.001785, Training accuracy: 99.96250\n",
            "Epoch, batch [33,  4000] loss: 0.002210, Training accuracy: 99.95000\n",
            "Epoch, batch [33,  6000] loss: 0.002139, Training accuracy: 99.95000\n",
            "Epoch, batch [33,  8000] loss: 0.001702, Training accuracy: 99.96250\n",
            "Epoch, batch [33, 10000] loss: 0.000568, Training accuracy: 100.00000\n",
            "Epoch, batch [33, 12000] loss: 0.002697, Training accuracy: 99.91250\n",
            "Epoch, batch [34,  2000] loss: 0.000853, Training accuracy: 99.98750\n",
            "Epoch, batch [34,  4000] loss: 0.001097, Training accuracy: 99.98750\n",
            "Epoch, batch [34,  6000] loss: 0.001120, Training accuracy: 99.97500\n",
            "Epoch, batch [34,  8000] loss: 0.001552, Training accuracy: 99.93750\n",
            "Epoch, batch [34, 10000] loss: 0.002057, Training accuracy: 99.96250\n",
            "Epoch, batch [34, 12000] loss: 0.002306, Training accuracy: 99.93750\n",
            "Epoch, batch [35,  2000] loss: 0.000642, Training accuracy: 100.00000\n",
            "Epoch, batch [35,  4000] loss: 0.002493, Training accuracy: 99.95000\n",
            "Epoch, batch [35,  6000] loss: 0.000749, Training accuracy: 99.97500\n",
            "Epoch, batch [35,  8000] loss: 0.001482, Training accuracy: 99.96250\n",
            "Epoch, batch [35, 10000] loss: 0.001534, Training accuracy: 99.96250\n",
            "Epoch, batch [35, 12000] loss: 0.001335, Training accuracy: 99.96250\n",
            "Epoch, batch [36,  2000] loss: 0.000481, Training accuracy: 100.00000\n",
            "Epoch, batch [36,  4000] loss: 0.000589, Training accuracy: 100.00000\n",
            "Epoch, batch [36,  6000] loss: 0.001550, Training accuracy: 99.95000\n",
            "Epoch, batch [36,  8000] loss: 0.000847, Training accuracy: 99.98750\n",
            "Epoch, batch [36, 10000] loss: 0.001156, Training accuracy: 99.97500\n",
            "Epoch, batch [36, 12000] loss: 0.001646, Training accuracy: 99.96250\n",
            "Epoch, batch [37,  2000] loss: 0.000446, Training accuracy: 100.00000\n",
            "Epoch, batch [37,  4000] loss: 0.001153, Training accuracy: 99.96250\n",
            "Epoch, batch [37,  6000] loss: 0.001627, Training accuracy: 99.97500\n",
            "Epoch, batch [37,  8000] loss: 0.000634, Training accuracy: 99.98750\n",
            "Epoch, batch [37, 10000] loss: 0.001178, Training accuracy: 99.96250\n",
            "Epoch, batch [37, 12000] loss: 0.000453, Training accuracy: 100.00000\n",
            "Epoch, batch [38,  2000] loss: 0.000898, Training accuracy: 99.97500\n",
            "Epoch, batch [38,  4000] loss: 0.000617, Training accuracy: 99.98750\n",
            "Epoch, batch [38,  6000] loss: 0.000519, Training accuracy: 99.98750\n",
            "Epoch, batch [38,  8000] loss: 0.001094, Training accuracy: 99.97500\n",
            "Epoch, batch [38, 10000] loss: 0.001093, Training accuracy: 99.96250\n",
            "Epoch, batch [38, 12000] loss: 0.000257, Training accuracy: 100.00000\n",
            "Epoch, batch [39,  2000] loss: 0.000351, Training accuracy: 100.00000\n",
            "Epoch, batch [39,  4000] loss: 0.000606, Training accuracy: 99.98750\n",
            "Epoch, batch [39,  6000] loss: 0.001222, Training accuracy: 99.95000\n",
            "Epoch, batch [39,  8000] loss: 0.000500, Training accuracy: 99.98750\n",
            "Epoch, batch [39, 10000] loss: 0.000362, Training accuracy: 100.00000\n",
            "Epoch, batch [39, 12000] loss: 0.000692, Training accuracy: 99.97500\n",
            "Epoch, batch [40,  2000] loss: 0.000563, Training accuracy: 99.98750\n",
            "Epoch, batch [40,  4000] loss: 0.000225, Training accuracy: 100.00000\n",
            "Epoch, batch [40,  6000] loss: 0.000761, Training accuracy: 99.98750\n",
            "Epoch, batch [40,  8000] loss: 0.000377, Training accuracy: 99.98750\n",
            "Epoch, batch [40, 10000] loss: 0.000395, Training accuracy: 99.98750\n",
            "Epoch, batch [40, 12000] loss: 0.000883, Training accuracy: 99.97500\n",
            "Epoch, batch [41,  2000] loss: 0.000428, Training accuracy: 99.98750\n",
            "Epoch, batch [41,  4000] loss: 0.000472, Training accuracy: 99.98750\n",
            "Epoch, batch [41,  6000] loss: 0.000258, Training accuracy: 100.00000\n",
            "Epoch, batch [41,  8000] loss: 0.000877, Training accuracy: 99.98750\n",
            "Epoch, batch [41, 10000] loss: 0.000616, Training accuracy: 99.98750\n",
            "Epoch, batch [41, 12000] loss: 0.000178, Training accuracy: 100.00000\n",
            "Epoch, batch [42,  2000] loss: 0.000296, Training accuracy: 100.00000\n",
            "Epoch, batch [42,  4000] loss: 0.000209, Training accuracy: 100.00000\n",
            "Epoch, batch [42,  6000] loss: 0.000783, Training accuracy: 99.97500\n",
            "Epoch, batch [42,  8000] loss: 0.000545, Training accuracy: 99.98750\n",
            "Epoch, batch [42, 10000] loss: 0.000471, Training accuracy: 99.98750\n",
            "Epoch, batch [42, 12000] loss: 0.000249, Training accuracy: 100.00000\n",
            "Epoch, batch [43,  2000] loss: 0.000239, Training accuracy: 100.00000\n",
            "Epoch, batch [43,  4000] loss: 0.000246, Training accuracy: 100.00000\n",
            "Epoch, batch [43,  6000] loss: 0.000381, Training accuracy: 100.00000\n",
            "Epoch, batch [43,  8000] loss: 0.000506, Training accuracy: 100.00000\n",
            "Epoch, batch [43, 10000] loss: 0.000361, Training accuracy: 99.98750\n",
            "Epoch, batch [43, 12000] loss: 0.000138, Training accuracy: 100.00000\n",
            "Epoch, batch [44,  2000] loss: 0.000178, Training accuracy: 100.00000\n",
            "Epoch, batch [44,  4000] loss: 0.000328, Training accuracy: 99.98750\n",
            "Epoch, batch [44,  6000] loss: 0.000425, Training accuracy: 100.00000\n",
            "Epoch, batch [44,  8000] loss: 0.000478, Training accuracy: 99.98750\n",
            "Epoch, batch [44, 10000] loss: 0.000186, Training accuracy: 100.00000\n",
            "Epoch, batch [44, 12000] loss: 0.000130, Training accuracy: 100.00000\n",
            "Epoch, batch [45,  2000] loss: 0.000364, Training accuracy: 100.00000\n",
            "Epoch, batch [45,  4000] loss: 0.000237, Training accuracy: 100.00000\n",
            "Epoch, batch [45,  6000] loss: 0.000501, Training accuracy: 99.98750\n",
            "Epoch, batch [45,  8000] loss: 0.000264, Training accuracy: 100.00000\n",
            "Epoch, batch [45, 10000] loss: 0.000148, Training accuracy: 100.00000\n",
            "Epoch, batch [45, 12000] loss: 0.000312, Training accuracy: 99.98750\n",
            "Epoch, batch [46,  2000] loss: 0.000407, Training accuracy: 100.00000\n",
            "Epoch, batch [46,  4000] loss: 0.000140, Training accuracy: 100.00000\n",
            "Epoch, batch [46,  6000] loss: 0.000340, Training accuracy: 99.98750\n",
            "Epoch, batch [46,  8000] loss: 0.000253, Training accuracy: 100.00000\n",
            "Epoch, batch [46, 10000] loss: 0.000113, Training accuracy: 100.00000\n",
            "Epoch, batch [46, 12000] loss: 0.000570, Training accuracy: 99.97500\n",
            "Epoch, batch [47,  2000] loss: 0.000107, Training accuracy: 100.00000\n",
            "Epoch, batch [47,  4000] loss: 0.000459, Training accuracy: 99.98750\n",
            "Epoch, batch [47,  6000] loss: 0.000314, Training accuracy: 100.00000\n",
            "Epoch, batch [47,  8000] loss: 0.000502, Training accuracy: 99.98750\n",
            "Epoch, batch [47, 10000] loss: 0.000090, Training accuracy: 100.00000\n",
            "Epoch, batch [47, 12000] loss: 0.000163, Training accuracy: 100.00000\n",
            "Epoch, batch [48,  2000] loss: 0.000130, Training accuracy: 100.00000\n",
            "Epoch, batch [48,  4000] loss: 0.000431, Training accuracy: 99.98750\n",
            "Epoch, batch [48,  6000] loss: 0.000075, Training accuracy: 100.00000\n",
            "Epoch, batch [48,  8000] loss: 0.000201, Training accuracy: 100.00000\n",
            "Epoch, batch [48, 10000] loss: 0.000248, Training accuracy: 100.00000\n",
            "Epoch, batch [48, 12000] loss: 0.000172, Training accuracy: 100.00000\n",
            "Epoch, batch [49,  2000] loss: 0.000230, Training accuracy: 100.00000\n",
            "Epoch, batch [49,  4000] loss: 0.000242, Training accuracy: 100.00000\n",
            "Epoch, batch [49,  6000] loss: 0.000290, Training accuracy: 99.98750\n",
            "Epoch, batch [49,  8000] loss: 0.000246, Training accuracy: 100.00000\n",
            "Epoch, batch [49, 10000] loss: 0.000123, Training accuracy: 100.00000\n",
            "Epoch, batch [49, 12000] loss: 0.000134, Training accuracy: 100.00000\n",
            "Epoch, batch [50,  2000] loss: 0.000240, Training accuracy: 100.00000\n",
            "Epoch, batch [50,  4000] loss: 0.000081, Training accuracy: 100.00000\n",
            "Epoch, batch [50,  6000] loss: 0.000082, Training accuracy: 100.00000\n",
            "Epoch, batch [50,  8000] loss: 0.000180, Training accuracy: 100.00000\n",
            "Epoch, batch [50, 10000] loss: 0.000262, Training accuracy: 99.98750\n",
            "Epoch, batch [50, 12000] loss: 0.000180, Training accuracy: 100.00000\n",
            "Finished training.\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets as ds\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms as ts\n",
        "import torchvision as tv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "from matplotlib import pyplot as plt\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "transform = ts.Compose(\n",
        "    [\n",
        "        ts.ToTensor(),\n",
        "        ts.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_set = ds.CIFAR10(root='./input/',\n",
        "                       train=True, transform=transform, download=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=4, shuffle=True, num_workers=0)\n",
        "\n",
        "testset = tv.datasets.CIFAR10(root='./input/',\n",
        "                              train=False, download=True, transform=transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset,\n",
        "                                         batch_size=4, shuffle=True, num_workers=0)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "class VGGNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(VGGNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),  # Conv1\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(32, 32, 3, padding=1),  # Conv2\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # Pool1\n",
        "            nn.Conv2d(32, 64, 3, padding=1),  # Conv3\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),  # Conv4\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # Pool2\n",
        "            nn.Conv2d(64, 128, 3, padding=1),  # Conv5\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),  # Conv6\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),  # Conv7\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # Pool3\n",
        "            nn.Conv2d(128, 256, 3, padding=1),  # Conv8\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),  # Conv9\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),  # Conv10\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # Pool4\n",
        "            nn.Conv2d(256, 256, 3, padding=1),  # Conv11\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),  # Conv12\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),  # Conv13\n",
        "            nn.ReLU(True),\n",
        "            # nn.MaxPool2d(2, 2)  # Pool5 是否还需要此池化层，每个通道的数据已经被降为1*1\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2 * 2 * 256, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, num_classes),\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "net = VGGNet(10)\n",
        "net.cuda()\n",
        "lr = 1e-3\n",
        "momentum = 0.9\n",
        "\n",
        "# num_epoch = 50\n",
        "num_epoch = 50\n",
        "\n",
        "critierion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
        "print('Training with learning rate = %f, momentum = %f ' % (lr, momentum))\n",
        "\n",
        "loss_p = np.array([])\n",
        "e = np.linspace(0, num_epoch, num_epoch)\n",
        "for t in range(num_epoch):\n",
        "    running_loss = 0\n",
        "    running_loss_sum_per_epoch = 0\n",
        "    total_images = 0\n",
        "    correct_images = 0\n",
        "    if t == 25:\n",
        "        optimizer = optim.SGD(net.parameters(), lr=lr/10, momentum=momentum)\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        images, labels = data\n",
        "        images, labels = images.cuda(), labels.cuda()\n",
        "        images = Variable(images)\n",
        "        labels = Variable(labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(images)\n",
        "        _, predicts = torch.max(outputs.data, 1)\n",
        "        loss = critierion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_images += labels.size(0)\n",
        "        correct_images += (predicts == labels).sum().item()\n",
        "        loss_data = loss.item()\n",
        "        running_loss += loss_data\n",
        "        \n",
        "        running_loss_sum_per_epoch = running_loss + running_loss_sum_per_epoch\n",
        "        if i % 2000 == 1999:\n",
        "            print('Epoch, batch [%d, %5d] loss: %.6f, Training accuracy: %.5f' %\n",
        "                  (t + 1, i + 1, running_loss / 2000, 100 * correct_images / total_images))\n",
        "            running_loss = 0\n",
        "            total_images = 0\n",
        "            correct_images = 0\n",
        "\n",
        "    loss_p = np.append(loss_p, running_loss_sum_per_epoch)\n",
        "\n",
        "print('Finished training.')\n",
        "'''\n",
        "plt.plot(e, loss_p, color='red', linestyle='--', labels='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "torch.save(net,'vgg16.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tcojtIqKmHP-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "430575fe-271d-4632-e3e7-ab055f491e52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81.69\n"
          ]
        }
      ],
      "source": [
        "#testloader\n",
        "from torchvision import datasets as ds\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms as ts\n",
        "import torchvision as tv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "from matplotlib import pyplot as plt\n",
        "import torch.backends.cudnn as cudnn\n",
        "vgg16_origin = torch.load('vgg16.pkl')\n",
        "total_images = 0\n",
        "correct_images = 0\n",
        "for i, data in enumerate(testloader, 0):\n",
        "    images, labels = data\n",
        "    images, labels = images.cuda(), labels.cuda()\n",
        "    images = Variable(images)\n",
        "    labels = Variable(labels)\n",
        "    \n",
        "    total_images += labels.size(0)\n",
        "    \n",
        "    outputs = vgg16_origin(images)\n",
        "    _, predicts = torch.max(outputs.data, 1)\n",
        "    \n",
        "    correct_images += (predicts == labels).sum().item()\n",
        "print(100 * correct_images / total_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6WJ6HWLnl1cl"
      },
      "outputs": [],
      "source": [
        "#Quantization Implementations\n",
        "import torch.quantization\n",
        "import torch.nn as tnn\n",
        "vgg16_origin = torch.load('vgg16.pkl')\n",
        "# create a model instance\n",
        "model_fp32 = vgg16_origin\n",
        "# create a quantized model instance\n",
        "model_int8 = torch.quantization.quantize_dynamic(model_fp32,{tnn.Conv2d}, dtype=torch.qint8) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3o_mchLhl4cH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bfd32f7-9a4a-43a3-b128-4e4dc9d35acc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81.74\n"
          ]
        }
      ],
      "source": [
        "total_images = 0\n",
        "correct_images = 0\n",
        "for i, data in enumerate(testloader, 0):\n",
        "    images, labels = data\n",
        "    images = Variable(images)\n",
        "    labels = Variable(labels)\n",
        "    images, labels = images.cuda(), labels.cuda()\n",
        "    total_images += labels.size(0)\n",
        "    \n",
        "    outputs = vgg16_origin(images)\n",
        "    _, predicts = torch.max(outputs.data, 1)\n",
        "    \n",
        "    correct_images += (predicts == labels).sum().item()\n",
        "print(100 * correct_images / total_images)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Cifar10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}