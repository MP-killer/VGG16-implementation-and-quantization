{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvcc —-version"
      ],
      "metadata": {
        "id": "rJ7p4W5HwHiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n"
      ],
      "metadata": {
        "id": "3O6rYuK6SiIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d153caf-e98a-4f2d-fa7c-ba504ea624b1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mnzDv5EHICQv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "677a1d5f-4214-42f3-e5d9-7cf38c9acade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Training with learning rate = 0.001000, momentum = 0.900000 \n",
            "Epoch, batch [1,  2000] loss: 2.302942, Training accuracy: 10.23750\n",
            "Epoch, batch [1,  4000] loss: 2.303306, Training accuracy: 9.87500\n",
            "Epoch, batch [1,  6000] loss: 2.303370, Training accuracy: 10.10000\n",
            "Epoch, batch [1,  8000] loss: 2.303334, Training accuracy: 9.41250\n",
            "Epoch, batch [1, 10000] loss: 2.303092, Training accuracy: 9.85000\n",
            "Epoch, batch [1, 12000] loss: 2.303216, Training accuracy: 10.27500\n",
            "Epoch, batch [2,  2000] loss: 2.302217, Training accuracy: 10.65000\n",
            "Epoch, batch [2,  4000] loss: 2.281831, Training accuracy: 12.28750\n",
            "Epoch, batch [2,  6000] loss: 2.186470, Training accuracy: 17.25000\n",
            "Epoch, batch [2,  8000] loss: 2.056736, Training accuracy: 18.18750\n",
            "Epoch, batch [2, 10000] loss: 1.967668, Training accuracy: 19.90000\n",
            "Epoch, batch [2, 12000] loss: 1.892167, Training accuracy: 24.38750\n",
            "Epoch, batch [3,  2000] loss: 1.804664, Training accuracy: 28.05000\n",
            "Epoch, batch [3,  4000] loss: 1.743312, Training accuracy: 30.46250\n",
            "Epoch, batch [3,  6000] loss: 1.694207, Training accuracy: 33.11250\n",
            "Epoch, batch [3,  8000] loss: 1.643290, Training accuracy: 34.58750\n",
            "Epoch, batch [3, 10000] loss: 1.601199, Training accuracy: 37.67500\n",
            "Epoch, batch [3, 12000] loss: 1.543331, Training accuracy: 41.21250\n",
            "Epoch, batch [4,  2000] loss: 1.446087, Training accuracy: 45.22500\n",
            "Epoch, batch [4,  4000] loss: 1.386318, Training accuracy: 48.57500\n",
            "Epoch, batch [4,  6000] loss: 1.341479, Training accuracy: 51.23750\n",
            "Epoch, batch [4,  8000] loss: 1.287658, Training accuracy: 53.23750\n",
            "Epoch, batch [4, 10000] loss: 1.253099, Training accuracy: 54.61250\n",
            "Epoch, batch [4, 12000] loss: 1.190390, Training accuracy: 57.27500\n",
            "Epoch, batch [5,  2000] loss: 1.111529, Training accuracy: 60.17500\n",
            "Epoch, batch [5,  4000] loss: 1.080655, Training accuracy: 61.47500\n",
            "Epoch, batch [5,  6000] loss: 1.069484, Training accuracy: 62.66250\n",
            "Epoch, batch [5,  8000] loss: 1.059325, Training accuracy: 62.85000\n",
            "Epoch, batch [5, 10000] loss: 1.005807, Training accuracy: 65.41250\n",
            "Epoch, batch [5, 12000] loss: 0.996133, Training accuracy: 65.15000\n",
            "Epoch, batch [6,  2000] loss: 0.910927, Training accuracy: 68.23750\n",
            "Epoch, batch [6,  4000] loss: 0.912693, Training accuracy: 68.85000\n",
            "Epoch, batch [6,  6000] loss: 0.907983, Training accuracy: 68.70000\n",
            "Epoch, batch [6,  8000] loss: 0.899048, Training accuracy: 69.25000\n",
            "Epoch, batch [6, 10000] loss: 0.869942, Training accuracy: 70.06250\n",
            "Epoch, batch [6, 12000] loss: 0.879401, Training accuracy: 70.07500\n",
            "Epoch, batch [7,  2000] loss: 0.778850, Training accuracy: 73.88750\n",
            "Epoch, batch [7,  4000] loss: 0.786198, Training accuracy: 73.50000\n",
            "Epoch, batch [7,  6000] loss: 0.770617, Training accuracy: 74.35000\n",
            "Epoch, batch [7,  8000] loss: 0.780935, Training accuracy: 73.53750\n",
            "Epoch, batch [7, 10000] loss: 0.770712, Training accuracy: 74.21250\n",
            "Epoch, batch [7, 12000] loss: 0.746366, Training accuracy: 74.83750\n",
            "Epoch, batch [8,  2000] loss: 0.657638, Training accuracy: 78.13750\n",
            "Epoch, batch [8,  4000] loss: 0.706053, Training accuracy: 76.20000\n",
            "Epoch, batch [8,  6000] loss: 0.691737, Training accuracy: 76.63750\n",
            "Epoch, batch [8,  8000] loss: 0.681999, Training accuracy: 77.40000\n",
            "Epoch, batch [8, 10000] loss: 0.661461, Training accuracy: 78.15000\n",
            "Epoch, batch [8, 12000] loss: 0.675633, Training accuracy: 77.83750\n",
            "Epoch, batch [9,  2000] loss: 0.567121, Training accuracy: 81.05000\n",
            "Epoch, batch [9,  4000] loss: 0.610108, Training accuracy: 79.96250\n",
            "Epoch, batch [9,  6000] loss: 0.590451, Training accuracy: 80.12500\n",
            "Epoch, batch [9,  8000] loss: 0.620855, Training accuracy: 79.58750\n",
            "Epoch, batch [9, 10000] loss: 0.614227, Training accuracy: 79.67500\n",
            "Epoch, batch [9, 12000] loss: 0.604974, Training accuracy: 79.55000\n",
            "Epoch, batch [10,  2000] loss: 0.501423, Training accuracy: 83.30000\n",
            "Epoch, batch [10,  4000] loss: 0.546371, Training accuracy: 82.26250\n",
            "Epoch, batch [10,  6000] loss: 0.543774, Training accuracy: 82.08750\n",
            "Epoch, batch [10,  8000] loss: 0.570520, Training accuracy: 80.70000\n",
            "Epoch, batch [10, 10000] loss: 0.533754, Training accuracy: 82.38750\n",
            "Epoch, batch [10, 12000] loss: 0.541288, Training accuracy: 82.22500\n",
            "Epoch, batch [11,  2000] loss: 0.443932, Training accuracy: 85.05000\n",
            "Epoch, batch [11,  4000] loss: 0.465726, Training accuracy: 84.76250\n",
            "Epoch, batch [11,  6000] loss: 0.499369, Training accuracy: 83.23750\n",
            "Epoch, batch [11,  8000] loss: 0.494754, Training accuracy: 83.45000\n",
            "Epoch, batch [11, 10000] loss: 0.507298, Training accuracy: 83.38750\n",
            "Epoch, batch [11, 12000] loss: 0.491479, Training accuracy: 83.23750\n",
            "Epoch, batch [12,  2000] loss: 0.406318, Training accuracy: 87.10000\n",
            "Epoch, batch [12,  4000] loss: 0.400052, Training accuracy: 86.77500\n",
            "Epoch, batch [12,  6000] loss: 0.440679, Training accuracy: 84.97500\n",
            "Epoch, batch [12,  8000] loss: 0.453659, Training accuracy: 85.40000\n",
            "Epoch, batch [12, 10000] loss: 0.457127, Training accuracy: 84.71250\n",
            "Epoch, batch [12, 12000] loss: 0.459879, Training accuracy: 85.25000\n",
            "Epoch, batch [13,  2000] loss: 0.364127, Training accuracy: 88.18750\n",
            "Epoch, batch [13,  4000] loss: 0.411628, Training accuracy: 86.23750\n",
            "Epoch, batch [13,  6000] loss: 0.398034, Training accuracy: 86.70000\n",
            "Epoch, batch [13,  8000] loss: 0.403549, Training accuracy: 86.30000\n",
            "Epoch, batch [13, 10000] loss: 0.412916, Training accuracy: 86.55000\n",
            "Epoch, batch [13, 12000] loss: 0.449095, Training accuracy: 85.56250\n",
            "Epoch, batch [14,  2000] loss: 0.342016, Training accuracy: 88.86250\n",
            "Epoch, batch [14,  4000] loss: 0.331889, Training accuracy: 89.05000\n",
            "Epoch, batch [14,  6000] loss: 0.362317, Training accuracy: 88.11250\n",
            "Epoch, batch [14,  8000] loss: 0.378569, Training accuracy: 87.56250\n",
            "Epoch, batch [14, 10000] loss: 0.404622, Training accuracy: 86.70000\n",
            "Epoch, batch [14, 12000] loss: 0.375299, Training accuracy: 87.47500\n",
            "Epoch, batch [15,  2000] loss: 0.268054, Training accuracy: 91.18750\n",
            "Epoch, batch [15,  4000] loss: 0.328637, Training accuracy: 89.63750\n",
            "Epoch, batch [15,  6000] loss: 0.329106, Training accuracy: 89.75000\n",
            "Epoch, batch [15,  8000] loss: 0.355006, Training accuracy: 88.56250\n",
            "Epoch, batch [15, 10000] loss: 0.350873, Training accuracy: 88.52500\n",
            "Epoch, batch [15, 12000] loss: 0.339999, Training accuracy: 88.45000\n",
            "Epoch, batch [16,  2000] loss: 0.255043, Training accuracy: 91.95000\n",
            "Epoch, batch [16,  4000] loss: 0.311173, Training accuracy: 89.97500\n",
            "Epoch, batch [16,  6000] loss: 0.317107, Training accuracy: 90.23750\n",
            "Epoch, batch [16,  8000] loss: 0.356348, Training accuracy: 88.42500\n",
            "Epoch, batch [16, 10000] loss: 0.328173, Training accuracy: 89.40000\n",
            "Epoch, batch [16, 12000] loss: 0.366998, Training accuracy: 88.05000\n",
            "Epoch, batch [17,  2000] loss: 0.276714, Training accuracy: 91.25000\n",
            "Epoch, batch [17,  4000] loss: 0.288704, Training accuracy: 90.86250\n",
            "Epoch, batch [17,  6000] loss: 0.295672, Training accuracy: 90.27500\n",
            "Epoch, batch [17,  8000] loss: 0.275588, Training accuracy: 90.66250\n",
            "Epoch, batch [17, 10000] loss: 0.318232, Training accuracy: 89.65000\n",
            "Epoch, batch [17, 12000] loss: 0.326549, Training accuracy: 89.57500\n",
            "Epoch, batch [18,  2000] loss: 0.239880, Training accuracy: 92.15000\n",
            "Epoch, batch [18,  4000] loss: 0.242565, Training accuracy: 92.27500\n",
            "Epoch, batch [18,  6000] loss: 0.260224, Training accuracy: 91.58750\n",
            "Epoch, batch [18,  8000] loss: 0.261346, Training accuracy: 91.50000\n",
            "Epoch, batch [18, 10000] loss: 0.260623, Training accuracy: 91.60000\n",
            "Epoch, batch [18, 12000] loss: 0.307913, Training accuracy: 90.41250\n",
            "Epoch, batch [19,  2000] loss: 0.213950, Training accuracy: 93.08750\n",
            "Epoch, batch [19,  4000] loss: 0.243614, Training accuracy: 91.85000\n",
            "Epoch, batch [19,  6000] loss: 0.230095, Training accuracy: 92.56250\n",
            "Epoch, batch [19,  8000] loss: 0.286762, Training accuracy: 90.91250\n",
            "Epoch, batch [19, 10000] loss: 0.256269, Training accuracy: 91.73750\n",
            "Epoch, batch [19, 12000] loss: 0.296574, Training accuracy: 90.48750\n",
            "Epoch, batch [20,  2000] loss: 0.211961, Training accuracy: 93.41250\n",
            "Epoch, batch [20,  4000] loss: 0.217142, Training accuracy: 93.27500\n",
            "Epoch, batch [20,  6000] loss: 0.230984, Training accuracy: 92.75000\n",
            "Epoch, batch [20,  8000] loss: 0.248038, Training accuracy: 91.93750\n",
            "Epoch, batch [20, 10000] loss: 0.267316, Training accuracy: 91.81250\n",
            "Epoch, batch [20, 12000] loss: 0.269905, Training accuracy: 91.70000\n",
            "Epoch, batch [21,  2000] loss: 0.206616, Training accuracy: 93.63750\n",
            "Epoch, batch [21,  4000] loss: 0.218738, Training accuracy: 93.28750\n",
            "Epoch, batch [21,  6000] loss: 0.233255, Training accuracy: 92.85000\n",
            "Epoch, batch [21,  8000] loss: 0.255642, Training accuracy: 92.15000\n",
            "Epoch, batch [21, 10000] loss: 0.264469, Training accuracy: 91.81250\n",
            "Epoch, batch [21, 12000] loss: 0.240990, Training accuracy: 92.10000\n",
            "Epoch, batch [22,  2000] loss: 0.207140, Training accuracy: 93.48750\n",
            "Epoch, batch [22,  4000] loss: 0.201792, Training accuracy: 93.93750\n",
            "Epoch, batch [22,  6000] loss: 0.247923, Training accuracy: 92.53750\n",
            "Epoch, batch [22,  8000] loss: 0.243366, Training accuracy: 92.18750\n",
            "Epoch, batch [22, 10000] loss: 0.260941, Training accuracy: 91.88750\n",
            "Epoch, batch [22, 12000] loss: 0.250141, Training accuracy: 92.25000\n",
            "Epoch, batch [23,  2000] loss: 0.182398, Training accuracy: 94.31250\n",
            "Epoch, batch [23,  4000] loss: 0.208661, Training accuracy: 93.51250\n",
            "Epoch, batch [23,  6000] loss: 0.235337, Training accuracy: 92.51250\n",
            "Epoch, batch [23,  8000] loss: 0.198459, Training accuracy: 93.52500\n",
            "Epoch, batch [23, 10000] loss: 0.251576, Training accuracy: 92.42500\n",
            "Epoch, batch [23, 12000] loss: 0.217727, Training accuracy: 92.95000\n",
            "Epoch, batch [24,  2000] loss: 0.198941, Training accuracy: 94.01250\n",
            "Epoch, batch [24,  4000] loss: 0.220503, Training accuracy: 93.31250\n",
            "Epoch, batch [24,  6000] loss: 0.200103, Training accuracy: 93.87500\n",
            "Epoch, batch [24,  8000] loss: 0.209905, Training accuracy: 93.83750\n",
            "Epoch, batch [24, 10000] loss: 0.241079, Training accuracy: 92.75000\n",
            "Epoch, batch [24, 12000] loss: 0.205013, Training accuracy: 93.61250\n",
            "Epoch, batch [25,  2000] loss: 0.166877, Training accuracy: 95.20000\n",
            "Epoch, batch [25,  4000] loss: 0.180538, Training accuracy: 94.52500\n",
            "Epoch, batch [25,  6000] loss: 0.191403, Training accuracy: 94.21250\n",
            "Epoch, batch [25,  8000] loss: 0.188653, Training accuracy: 94.30000\n",
            "Epoch, batch [25, 10000] loss: 0.223693, Training accuracy: 93.38750\n",
            "Epoch, batch [25, 12000] loss: 0.228583, Training accuracy: 93.06250\n",
            "Epoch, batch [26,  2000] loss: 0.095523, Training accuracy: 97.18750\n",
            "Epoch, batch [26,  4000] loss: 0.067391, Training accuracy: 98.13750\n",
            "Epoch, batch [26,  6000] loss: 0.061848, Training accuracy: 98.02500\n",
            "Epoch, batch [26,  8000] loss: 0.058541, Training accuracy: 98.26250\n",
            "Epoch, batch [26, 10000] loss: 0.055367, Training accuracy: 98.43750\n",
            "Epoch, batch [26, 12000] loss: 0.049945, Training accuracy: 98.63750\n",
            "Epoch, batch [27,  2000] loss: 0.020875, Training accuracy: 99.56250\n",
            "Epoch, batch [27,  4000] loss: 0.021798, Training accuracy: 99.45000\n",
            "Epoch, batch [27,  6000] loss: 0.017317, Training accuracy: 99.56250\n",
            "Epoch, batch [27,  8000] loss: 0.025149, Training accuracy: 99.27500\n",
            "Epoch, batch [27, 10000] loss: 0.017429, Training accuracy: 99.56250\n",
            "Epoch, batch [27, 12000] loss: 0.017223, Training accuracy: 99.48750\n",
            "Epoch, batch [28,  2000] loss: 0.009891, Training accuracy: 99.77500\n",
            "Epoch, batch [28,  4000] loss: 0.005632, Training accuracy: 99.88750\n",
            "Epoch, batch [28,  6000] loss: 0.009138, Training accuracy: 99.76250\n",
            "Epoch, batch [28,  8000] loss: 0.008767, Training accuracy: 99.81250\n",
            "Epoch, batch [28, 10000] loss: 0.008939, Training accuracy: 99.75000\n",
            "Epoch, batch [28, 12000] loss: 0.006157, Training accuracy: 99.86250\n",
            "Epoch, batch [29,  2000] loss: 0.003457, Training accuracy: 99.95000\n",
            "Epoch, batch [29,  4000] loss: 0.005550, Training accuracy: 99.86250\n",
            "Epoch, batch [29,  6000] loss: 0.004268, Training accuracy: 99.90000\n",
            "Epoch, batch [29,  8000] loss: 0.006660, Training accuracy: 99.91250\n",
            "Epoch, batch [29, 10000] loss: 0.003590, Training accuracy: 99.95000\n",
            "Epoch, batch [29, 12000] loss: 0.004887, Training accuracy: 99.88750\n",
            "Epoch, batch [30,  2000] loss: 0.003812, Training accuracy: 99.92500\n",
            "Epoch, batch [30,  4000] loss: 0.003360, Training accuracy: 99.93750\n",
            "Epoch, batch [30,  6000] loss: 0.002824, Training accuracy: 99.95000\n",
            "Epoch, batch [30,  8000] loss: 0.003087, Training accuracy: 99.93750\n",
            "Epoch, batch [30, 10000] loss: 0.001983, Training accuracy: 99.97500\n",
            "Epoch, batch [30, 12000] loss: 0.003383, Training accuracy: 99.91250\n",
            "Epoch, batch [31,  2000] loss: 0.002415, Training accuracy: 99.95000\n",
            "Epoch, batch [31,  4000] loss: 0.001738, Training accuracy: 99.97500\n",
            "Epoch, batch [31,  6000] loss: 0.001733, Training accuracy: 99.96250\n",
            "Epoch, batch [31,  8000] loss: 0.001767, Training accuracy: 99.96250\n",
            "Epoch, batch [31, 10000] loss: 0.001874, Training accuracy: 99.96250\n",
            "Epoch, batch [31, 12000] loss: 0.002055, Training accuracy: 99.96250\n",
            "Epoch, batch [32,  2000] loss: 0.001393, Training accuracy: 99.98750\n",
            "Epoch, batch [32,  4000] loss: 0.001204, Training accuracy: 99.98750\n",
            "Epoch, batch [32,  6000] loss: 0.002462, Training accuracy: 99.95000\n",
            "Epoch, batch [32,  8000] loss: 0.001710, Training accuracy: 99.98750\n",
            "Epoch, batch [32, 10000] loss: 0.001184, Training accuracy: 99.96250\n",
            "Epoch, batch [32, 12000] loss: 0.000976, Training accuracy: 99.98750\n",
            "Epoch, batch [33,  2000] loss: 0.001450, Training accuracy: 99.96250\n",
            "Epoch, batch [33,  4000] loss: 0.001430, Training accuracy: 99.95000\n",
            "Epoch, batch [33,  6000] loss: 0.001344, Training accuracy: 99.97500\n",
            "Epoch, batch [33,  8000] loss: 0.000629, Training accuracy: 100.00000\n",
            "Epoch, batch [33, 10000] loss: 0.000976, Training accuracy: 99.97500\n",
            "Epoch, batch [33, 12000] loss: 0.002044, Training accuracy: 99.97500\n",
            "Epoch, batch [34,  2000] loss: 0.000853, Training accuracy: 100.00000\n",
            "Epoch, batch [34,  4000] loss: 0.000786, Training accuracy: 100.00000\n",
            "Epoch, batch [34,  6000] loss: 0.001029, Training accuracy: 99.97500\n",
            "Epoch, batch [34,  8000] loss: 0.000550, Training accuracy: 100.00000\n",
            "Epoch, batch [34, 10000] loss: 0.000754, Training accuracy: 99.98750\n",
            "Epoch, batch [34, 12000] loss: 0.001138, Training accuracy: 99.97500\n",
            "Epoch, batch [35,  2000] loss: 0.000595, Training accuracy: 100.00000\n",
            "Epoch, batch [35,  4000] loss: 0.000949, Training accuracy: 99.98750\n",
            "Epoch, batch [35,  6000] loss: 0.000825, Training accuracy: 99.98750\n",
            "Epoch, batch [35,  8000] loss: 0.000371, Training accuracy: 100.00000\n",
            "Epoch, batch [35, 10000] loss: 0.000624, Training accuracy: 99.98750\n",
            "Epoch, batch [35, 12000] loss: 0.000498, Training accuracy: 100.00000\n",
            "Epoch, batch [36,  2000] loss: 0.000482, Training accuracy: 100.00000\n",
            "Epoch, batch [36,  4000] loss: 0.000781, Training accuracy: 99.98750\n",
            "Epoch, batch [36,  6000] loss: 0.000421, Training accuracy: 100.00000\n",
            "Epoch, batch [36,  8000] loss: 0.000416, Training accuracy: 100.00000\n",
            "Epoch, batch [36, 10000] loss: 0.000591, Training accuracy: 99.98750\n",
            "Epoch, batch [36, 12000] loss: 0.000629, Training accuracy: 99.98750\n",
            "Epoch, batch [37,  2000] loss: 0.000679, Training accuracy: 99.98750\n",
            "Epoch, batch [37,  4000] loss: 0.000452, Training accuracy: 100.00000\n",
            "Epoch, batch [37,  6000] loss: 0.000443, Training accuracy: 100.00000\n",
            "Epoch, batch [37,  8000] loss: 0.000571, Training accuracy: 100.00000\n",
            "Epoch, batch [37, 10000] loss: 0.000417, Training accuracy: 100.00000\n",
            "Epoch, batch [37, 12000] loss: 0.000350, Training accuracy: 100.00000\n",
            "Epoch, batch [38,  2000] loss: 0.000549, Training accuracy: 99.98750\n",
            "Epoch, batch [38,  4000] loss: 0.000390, Training accuracy: 100.00000\n",
            "Epoch, batch [38,  6000] loss: 0.000499, Training accuracy: 99.98750\n",
            "Epoch, batch [38,  8000] loss: 0.000455, Training accuracy: 99.98750\n",
            "Epoch, batch [38, 10000] loss: 0.000297, Training accuracy: 100.00000\n",
            "Epoch, batch [38, 12000] loss: 0.000415, Training accuracy: 100.00000\n",
            "Epoch, batch [39,  2000] loss: 0.000388, Training accuracy: 100.00000\n",
            "Epoch, batch [39,  4000] loss: 0.000462, Training accuracy: 99.98750\n",
            "Epoch, batch [39,  6000] loss: 0.000271, Training accuracy: 100.00000\n",
            "Epoch, batch [39,  8000] loss: 0.000295, Training accuracy: 100.00000\n",
            "Epoch, batch [39, 10000] loss: 0.000449, Training accuracy: 100.00000\n",
            "Epoch, batch [39, 12000] loss: 0.000213, Training accuracy: 100.00000\n",
            "Epoch, batch [40,  2000] loss: 0.000423, Training accuracy: 99.98750\n",
            "Epoch, batch [40,  4000] loss: 0.000173, Training accuracy: 100.00000\n",
            "Epoch, batch [40,  6000] loss: 0.000191, Training accuracy: 100.00000\n",
            "Epoch, batch [40,  8000] loss: 0.000187, Training accuracy: 100.00000\n",
            "Epoch, batch [40, 10000] loss: 0.000427, Training accuracy: 99.98750\n",
            "Epoch, batch [40, 12000] loss: 0.000252, Training accuracy: 100.00000\n",
            "Epoch, batch [41,  2000] loss: 0.000350, Training accuracy: 100.00000\n",
            "Epoch, batch [41,  4000] loss: 0.000376, Training accuracy: 99.98750\n",
            "Epoch, batch [41,  6000] loss: 0.000167, Training accuracy: 100.00000\n",
            "Epoch, batch [41,  8000] loss: 0.000234, Training accuracy: 100.00000\n",
            "Epoch, batch [41, 10000] loss: 0.000215, Training accuracy: 100.00000\n",
            "Epoch, batch [41, 12000] loss: 0.000115, Training accuracy: 100.00000\n",
            "Epoch, batch [42,  2000] loss: 0.000314, Training accuracy: 100.00000\n",
            "Epoch, batch [42,  4000] loss: 0.000138, Training accuracy: 100.00000\n",
            "Epoch, batch [42,  6000] loss: 0.000234, Training accuracy: 100.00000\n",
            "Epoch, batch [42,  8000] loss: 0.000228, Training accuracy: 100.00000\n",
            "Epoch, batch [42, 10000] loss: 0.000195, Training accuracy: 100.00000\n",
            "Epoch, batch [42, 12000] loss: 0.000200, Training accuracy: 100.00000\n",
            "Epoch, batch [43,  2000] loss: 0.000186, Training accuracy: 100.00000\n",
            "Epoch, batch [43,  4000] loss: 0.000187, Training accuracy: 100.00000\n",
            "Epoch, batch [43,  6000] loss: 0.000212, Training accuracy: 100.00000\n",
            "Epoch, batch [43,  8000] loss: 0.000116, Training accuracy: 100.00000\n",
            "Epoch, batch [43, 10000] loss: 0.000204, Training accuracy: 100.00000\n",
            "Epoch, batch [43, 12000] loss: 0.000320, Training accuracy: 100.00000\n",
            "Epoch, batch [44,  2000] loss: 0.000298, Training accuracy: 100.00000\n",
            "Epoch, batch [44,  4000] loss: 0.000177, Training accuracy: 100.00000\n",
            "Epoch, batch [44,  6000] loss: 0.000144, Training accuracy: 100.00000\n",
            "Epoch, batch [44,  8000] loss: 0.000329, Training accuracy: 99.98750\n",
            "Epoch, batch [44, 10000] loss: 0.000140, Training accuracy: 100.00000\n",
            "Epoch, batch [44, 12000] loss: 0.000146, Training accuracy: 100.00000\n",
            "Epoch, batch [45,  2000] loss: 0.000156, Training accuracy: 100.00000\n",
            "Epoch, batch [45,  4000] loss: 0.000213, Training accuracy: 100.00000\n",
            "Epoch, batch [45,  6000] loss: 0.000171, Training accuracy: 100.00000\n",
            "Epoch, batch [45,  8000] loss: 0.000179, Training accuracy: 100.00000\n",
            "Epoch, batch [45, 10000] loss: 0.000169, Training accuracy: 100.00000\n",
            "Epoch, batch [45, 12000] loss: 0.000207, Training accuracy: 100.00000\n",
            "Epoch, batch [46,  2000] loss: 0.000096, Training accuracy: 100.00000\n",
            "Epoch, batch [46,  4000] loss: 0.000170, Training accuracy: 100.00000\n",
            "Epoch, batch [46,  6000] loss: 0.000210, Training accuracy: 100.00000\n",
            "Epoch, batch [46,  8000] loss: 0.000147, Training accuracy: 100.00000\n",
            "Epoch, batch [46, 10000] loss: 0.000141, Training accuracy: 100.00000\n",
            "Epoch, batch [46, 12000] loss: 0.000131, Training accuracy: 100.00000\n",
            "Epoch, batch [47,  2000] loss: 0.000124, Training accuracy: 100.00000\n",
            "Epoch, batch [47,  4000] loss: 0.000119, Training accuracy: 100.00000\n",
            "Epoch, batch [47,  6000] loss: 0.000222, Training accuracy: 100.00000\n",
            "Epoch, batch [47,  8000] loss: 0.000152, Training accuracy: 100.00000\n",
            "Epoch, batch [47, 10000] loss: 0.000146, Training accuracy: 100.00000\n",
            "Epoch, batch [47, 12000] loss: 0.000126, Training accuracy: 100.00000\n",
            "Epoch, batch [48,  2000] loss: 0.000125, Training accuracy: 100.00000\n",
            "Epoch, batch [48,  4000] loss: 0.000163, Training accuracy: 100.00000\n",
            "Epoch, batch [48,  6000] loss: 0.000128, Training accuracy: 100.00000\n",
            "Epoch, batch [48,  8000] loss: 0.000106, Training accuracy: 100.00000\n",
            "Epoch, batch [48, 10000] loss: 0.000181, Training accuracy: 100.00000\n",
            "Epoch, batch [48, 12000] loss: 0.000128, Training accuracy: 100.00000\n",
            "Epoch, batch [49,  2000] loss: 0.000171, Training accuracy: 100.00000\n",
            "Epoch, batch [49,  4000] loss: 0.000111, Training accuracy: 100.00000\n",
            "Epoch, batch [49,  6000] loss: 0.000140, Training accuracy: 100.00000\n",
            "Epoch, batch [49,  8000] loss: 0.000088, Training accuracy: 100.00000\n",
            "Epoch, batch [49, 10000] loss: 0.000094, Training accuracy: 100.00000\n",
            "Epoch, batch [49, 12000] loss: 0.000187, Training accuracy: 100.00000\n",
            "Epoch, batch [50,  2000] loss: 0.000159, Training accuracy: 100.00000\n",
            "Epoch, batch [50,  4000] loss: 0.000088, Training accuracy: 100.00000\n",
            "Epoch, batch [50,  6000] loss: 0.000097, Training accuracy: 100.00000\n",
            "Epoch, batch [50,  8000] loss: 0.000092, Training accuracy: 100.00000\n",
            "Epoch, batch [50, 10000] loss: 0.000128, Training accuracy: 100.00000\n",
            "Epoch, batch [50, 12000] loss: 0.000114, Training accuracy: 100.00000\n",
            "Finished training.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c8dd1f64-3be9-4cdf-9b0e-bd19d975eff2\", \"vgg16.pkl\", 17908293)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from torchvision import datasets as ds\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms as ts\n",
        "import torchvision as tv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "from matplotlib import pyplot as plt\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "transform = ts.Compose(\n",
        "    [\n",
        "        ts.ToTensor(),\n",
        "        ts.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_set = ds.CIFAR10(root='./input/',\n",
        "                       train=True, transform=transform, download=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=4, shuffle=True, num_workers=0)\n",
        "\n",
        "testset = tv.datasets.CIFAR10(root='./input/',\n",
        "                              train=False, download=True, transform=transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset,\n",
        "                                         batch_size=4, shuffle=True, num_workers=0)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "class VGGNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(VGGNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),  # Conv1\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(32, 32, 3, padding=1),  # Conv2\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # Pool1\n",
        "            nn.Conv2d(32, 64, 3, padding=1),  # Conv3\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),  # Conv4\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # Pool2\n",
        "            nn.Conv2d(64, 128, 3, padding=1),  # Conv5\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),  # Conv6\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),  # Conv7\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # Pool3\n",
        "            nn.Conv2d(128, 256, 3, padding=1),  # Conv8\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),  # Conv9\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),  # Conv10\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, 2),  # Pool4\n",
        "            nn.Conv2d(256, 256, 3, padding=1),  # Conv11\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),  # Conv12\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),  # Conv13\n",
        "            nn.ReLU(True),\n",
        "            # nn.MaxPool2d(2, 2)  # Pool5 是否还需要此池化层，每个通道的数据已经被降为1*1\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2 * 2 * 256, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, num_classes),\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "net = VGGNet(10)\n",
        "net.cuda()\n",
        "lr = 1e-3\n",
        "momentum = 0.9\n",
        "\n",
        "# num_epoch = 50\n",
        "num_epoch = 50\n",
        "\n",
        "critierion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
        "print('Training with learning rate = %f, momentum = %f ' % (lr, momentum))\n",
        "\n",
        "loss_p = np.array([])\n",
        "e = np.linspace(0, num_epoch, num_epoch)\n",
        "for t in range(num_epoch):\n",
        "    running_loss = 0\n",
        "    running_loss_sum_per_epoch = 0\n",
        "    total_images = 0\n",
        "    correct_images = 0\n",
        "    if t == 25:\n",
        "        optimizer = optim.SGD(net.parameters(), lr=lr/10, momentum=momentum)\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        images, labels = data\n",
        "        images, labels = images.cuda(), labels.cuda()\n",
        "        images = Variable(images)\n",
        "        labels = Variable(labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(images)\n",
        "        _, predicts = torch.max(outputs.data, 1)\n",
        "        loss = critierion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_images += labels.size(0)\n",
        "        correct_images += (predicts == labels).sum().item()\n",
        "        loss_data = loss.item()\n",
        "        running_loss += loss_data\n",
        "        \n",
        "        running_loss_sum_per_epoch = running_loss + running_loss_sum_per_epoch\n",
        "        if i % 2000 == 1999:\n",
        "            print('Epoch, batch [%d, %5d] loss: %.6f, Training accuracy: %.5f' %\n",
        "                  (t + 1, i + 1, running_loss / 2000, 100 * correct_images / total_images))\n",
        "            running_loss = 0\n",
        "            total_images = 0\n",
        "            correct_images = 0\n",
        "\n",
        "    loss_p = np.append(loss_p, running_loss_sum_per_epoch)\n",
        "\n",
        "print('Finished training.')\n",
        "'''\n",
        "plt.plot(e, loss_p, color='red', linestyle='--', labels='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "torch.save(net,'vgg16.pkl')\n",
        "from google.colab import files\n",
        "files.download('vgg16.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tcojtIqKmHP-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7afa02ee-189b-4678-e04e-03cfe95c228e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81.55\n"
          ]
        }
      ],
      "source": [
        "#testloader\n",
        "from torchvision import datasets as ds\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms as ts\n",
        "import torchvision as tv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "from matplotlib import pyplot as plt\n",
        "import torch.backends.cudnn as cudnn\n",
        "vgg16_origin = torch.load('vgg16.pkl')\n",
        "total_images = 0\n",
        "correct_images = 0\n",
        "for i, data in enumerate(testloader, 0):\n",
        "    images, labels = data\n",
        "    images, labels = images.cuda(), labels.cuda()\n",
        "    images = Variable(images)\n",
        "    labels = Variable(labels)\n",
        "    \n",
        "    total_images += labels.size(0)\n",
        "    \n",
        "    outputs = vgg16_origin(images)\n",
        "    _, predicts = torch.max(outputs.data, 1)\n",
        "    \n",
        "    correct_images += (predicts == labels).sum().item()\n",
        "print(100 * correct_images / total_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6WJ6HWLnl1cl"
      },
      "outputs": [],
      "source": [
        "#Quantization Implementations\n",
        "import torch.quantization\n",
        "import torch.nn as tnn\n",
        "vgg16_origin = torch.load('vgg16.pkl')\n",
        "# create a model instance\n",
        "model_fp32 = vgg16_origin\n",
        "# create a quantized model instance\n",
        "model_int8 = torch.quantization.quantize_dynamic(model_fp32,{tnn.Conv2d}, dtype=torch.qint8) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3o_mchLhl4cH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c4cb423-fc5b-400e-c9a6-a8b6907eaec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81.47\n"
          ]
        }
      ],
      "source": [
        "total_images = 0\n",
        "correct_images = 0\n",
        "for i, data in enumerate(testloader, 0):\n",
        "    images, labels = data\n",
        "    images = Variable(images)\n",
        "    labels = Variable(labels)\n",
        "    images, labels = images.cuda(), labels.cuda()\n",
        "    total_images += labels.size(0)\n",
        "    \n",
        "    outputs = model_int8(images)\n",
        "    _, predicts = torch.max(outputs.data, 1)\n",
        "    \n",
        "    correct_images += (predicts == labels).sum().item()\n",
        "print(100 * correct_images / total_images)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "vgg16.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}